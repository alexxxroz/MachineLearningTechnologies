{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**I. Perform any necessary preprocessing on the text, including converting to lower case, removing stop words, numbers or non-alphabetic characters, lemmatization.**"
      ],
      "metadata": {
        "id": "kqoWTVVN0Lk_"
      },
      "id": "kqoWTVVN0Lk_"
    },
    {
      "cell_type": "code",
      "execution_count": 157,
      "id": "a59127fd",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "a59127fd",
        "outputId": "78af9bc2-043e-4909-d34b-eaea52dbcb03"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'CHAPTER I.Down the R'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 157
        }
      ],
      "source": [
        "f = open('Alice.txt','r')\n",
        "raw_text = f.read().replace('\\n', '')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "id": "0f8dfd90",
      "metadata": {
        "id": "0f8dfd90"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "text = str(raw_text).lower() #to lower-case\n",
        "text = re.sub(\"\\d+\", \"\", str(text)) #remove numbers\n",
        "regex = re.compile('[^a-zA-Z]') #remove everything except letters\n",
        "text = regex.sub(' ', text)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download(\"wordnet\")\n",
        "nltk.download('omw-1.4')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nbt27Xsmy-sc",
        "outputId": "1fa6affe-c271-4258-afa6-b1a2fe58a530"
      },
      "id": "nbt27Xsmy-sc",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "7bcc8917",
      "metadata": {
        "id": "7bcc8917"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer, WhitespaceTokenizer\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "stop_words = stopwords.words(\"english\")\n",
        "tokens = WhitespaceTokenizer().tokenize(text)\n",
        "tokens = [token for token in tokens if token not in stop_words]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lems = [lemmatizer.lemmatize(token) for token in tokens]\n",
        "lems[:40]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IBYTpzbYzEru",
        "outputId": "fe619c3e-0c90-45a2-d387-bae1f5409dee"
      },
      "id": "IBYTpzbYzEru",
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['chapter',\n",
              " 'rabbit',\n",
              " 'holealice',\n",
              " 'beginning',\n",
              " 'get',\n",
              " 'tired',\n",
              " 'sitting',\n",
              " 'sister',\n",
              " 'thebank',\n",
              " 'nothing',\n",
              " 'twice',\n",
              " 'peeped',\n",
              " 'intothe',\n",
              " 'book',\n",
              " 'sister',\n",
              " 'reading',\n",
              " 'picture',\n",
              " 'orconversations',\n",
              " 'use',\n",
              " 'book',\n",
              " 'thought',\n",
              " 'alice',\n",
              " 'without',\n",
              " 'picture',\n",
              " 'conversation',\n",
              " 'considering',\n",
              " 'mind',\n",
              " 'well',\n",
              " 'could',\n",
              " 'thehot',\n",
              " 'day',\n",
              " 'made',\n",
              " 'feel',\n",
              " 'sleepy',\n",
              " 'stupid',\n",
              " 'whether',\n",
              " 'pleasure',\n",
              " 'ofmaking',\n",
              " 'daisy',\n",
              " 'chain']"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**II. Find Top 10 most important (for example, in terms of TF-IDF metric) words from each chapter in the text (not \"Alice\"). How would you name each chapter according to the identified tokens?**"
      ],
      "metadata": {
        "id": "bY8IyjDj0FTs"
      },
      "id": "bY8IyjDj0FTs"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Creating a dict of chapters\"\"\"\n",
        "chapters, c = {}, 1\n",
        "for i,lem in enumerate(lems):\n",
        "  if lem == 'chapter' and i != 0:\n",
        "    chapters[f'Chapter {c}'] = lems[:i-1]\n",
        "    c+=1\n",
        "  if c == 12 and lem == 'chapter':\n",
        "    chapters[f'Chapter {c}'] = lems[i:]"
      ],
      "metadata": {
        "id": "T5fxlhR5zsL2"
      },
      "id": "T5fxlhR5zsL2",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Defining function to calculate TF-IDF\"\"\"\n",
        "def TF_IDF(txt):\n",
        "  TF_IDF, k = {}, 0\n",
        "  tokens = np.array(txt)\n",
        "  for j, i in enumerate(np.unique(txt)):\n",
        "    if i != 'alice':\n",
        "\n",
        "      _, N = np.unique(txt, return_counts = True)\n",
        "      TF = N[j]/len(txt)\n",
        "\n",
        "      for w in range(1,13):\n",
        "        if w in chapters[f'Chapter {w}']:\n",
        "          k += 1\n",
        "      IDF = np.log(12/(k+1))\n",
        "    TF_IDF[i] = TF*IDF\n",
        "  sort = sorted(TF_IDF.items(), key=lambda x:x[1], reverse=True)\n",
        "\n",
        "  return sort[:10]\n",
        "\n",
        "def show_TFIDF(D):\n",
        "  for i in range(1,13):\n",
        "    v1 = TF_IDF(chapters[f'Chapter {i}'])\n",
        "    print(\"-\"*18, f'\\nChapter {i} top:\\n')\n",
        "    for k, v in v1:\n",
        "      print(k, v, '\\n')"
      ],
      "metadata": {
        "id": "L0XLC-Wj7lxB"
      },
      "id": "L0XLC-Wj7lxB",
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "show_TFIDF(chapters)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IHn2uW4KEwY1",
        "outputId": "fad02c99-2268-4cae-b535-23fd03858a9d"
      },
      "id": "IHn2uW4KEwY1",
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "------------------ \n",
            "Chapter 1 top:\n",
            "\n",
            "little 0.036351821418006275 \n",
            "\n",
            "like 0.02856214539986207 \n",
            "\n",
            "way 0.02856214539986207 \n",
            "\n",
            "see 0.02596558672714734 \n",
            "\n",
            "one 0.023369028054432608 \n",
            "\n",
            "rabbit 0.023369028054432608 \n",
            "\n",
            "think 0.023369028054432608 \n",
            "\n",
            "could 0.018175910709003137 \n",
            "\n",
            "door 0.018175910709003137 \n",
            "\n",
            "eat 0.018175910709003137 \n",
            "\n",
            "------------------ \n",
            "Chapter 2 top:\n",
            "\n",
            "little 0.0388468991629182 \n",
            "\n",
            "mouse 0.028487726052806674 \n",
            "\n",
            "said 0.024603036136514857 \n",
            "\n",
            "like 0.023308139497750915 \n",
            "\n",
            "way 0.023308139497750915 \n",
            "\n",
            "one 0.022013242858986974 \n",
            "\n",
            "oh 0.020718346220223033 \n",
            "\n",
            "see 0.0194234495814591 \n",
            "\n",
            "thing 0.0194234495814591 \n",
            "\n",
            "thought 0.0194234495814591 \n",
            "\n",
            "------------------ \n",
            "Chapter 3 top:\n",
            "\n",
            "said 0.04747066340520795 \n",
            "\n",
            "mouse 0.038341689673437186 \n",
            "\n",
            "little 0.0301256133148435 \n",
            "\n",
            "one 0.020996639583072744 \n",
            "\n",
            "like 0.019170844836718593 \n",
            "\n",
            "thing 0.019170844836718593 \n",
            "\n",
            "way 0.018257947463541518 \n",
            "\n",
            "could 0.01734505009036444 \n",
            "\n",
            "oh 0.01734505009036444 \n",
            "\n",
            "see 0.016432152717187364 \n",
            "\n",
            "------------------ \n",
            "Chapter 4 top:\n",
            "\n",
            "said 0.0417631369712269 \n",
            "\n",
            "little 0.031638740129717347 \n",
            "\n",
            "mouse 0.027842091314151263 \n",
            "\n",
            "one 0.02277989289339649 \n",
            "\n",
            "thing 0.017717694472641713 \n",
            "\n",
            "like 0.01708491967004737 \n",
            "\n",
            "oh 0.016452144867453018 \n",
            "\n",
            "rabbit 0.016452144867453018 \n",
            "\n",
            "thought 0.016452144867453018 \n",
            "\n",
            "way 0.016452144867453018 \n",
            "\n",
            "------------------ \n",
            "Chapter 5 top:\n",
            "\n",
            "said 0.05976742451589565 \n",
            "\n",
            "little 0.03039021585554016 \n",
            "\n",
            "mouse 0.02228615829406278 \n",
            "\n",
            "one 0.02177965469647045 \n",
            "\n",
            "like 0.016208115122954753 \n",
            "\n",
            "thing 0.015701611525362414 \n",
            "\n",
            "thought 0.01519510792777008 \n",
            "\n",
            "see 0.014688604330177744 \n",
            "\n",
            "time 0.014688604330177744 \n",
            "\n",
            "oh 0.014182100732585409 \n",
            "\n",
            "------------------ \n",
            "Chapter 6 top:\n",
            "\n",
            "said 0.06679637609657958 \n",
            "\n",
            "little 0.02891794331010458 \n",
            "\n",
            "one 0.019957453833734148 \n",
            "\n",
            "like 0.019550158857535492 \n",
            "\n",
            "mouse 0.017920978952740866 \n",
            "\n",
            "thought 0.015884504071747584 \n",
            "\n",
            "see 0.01547720909554893 \n",
            "\n",
            "thing 0.01547720909554893 \n",
            "\n",
            "cat 0.014662619143151617 \n",
            "\n",
            "could 0.014662619143151617 \n",
            "\n",
            "------------------ \n",
            "Chapter 7 top:\n",
            "\n",
            "said 0.07604388134001393 \n",
            "\n",
            "little 0.026615358469004872 \n",
            "\n",
            "one 0.020393586359367367 \n",
            "\n",
            "like 0.018665316328912506 \n",
            "\n",
            "thing 0.016937046298457645 \n",
            "\n",
            "time 0.016937046298457645 \n",
            "\n",
            "know 0.016591392292366672 \n",
            "\n",
            "went 0.015554430274093757 \n",
            "\n",
            "mouse 0.015208776268002783 \n",
            "\n",
            "thought 0.014863122261911811 \n",
            "\n",
            "------------------ \n",
            "Chapter 8 top:\n",
            "\n",
            "said 0.07820366873807282 \n",
            "\n",
            "little 0.02387898282078559 \n",
            "\n",
            "one 0.01970016082714811 \n",
            "\n",
            "like 0.018207724400849012 \n",
            "\n",
            "went 0.01581982611877045 \n",
            "\n",
            "thing 0.015521338833510633 \n",
            "\n",
            "time 0.015521338833510633 \n",
            "\n",
            "see 0.015222851548250812 \n",
            "\n",
            "know 0.014924364262990993 \n",
            "\n",
            "thought 0.014924364262990993 \n",
            "\n",
            "------------------ \n",
            "Chapter 9 top:\n",
            "\n",
            "said 0.08371935465860306 \n",
            "\n",
            "little 0.02297660522176172 \n",
            "\n",
            "one 0.018751022652242325 \n",
            "\n",
            "like 0.0182228248310524 \n",
            "\n",
            "went 0.01743052809926751 \n",
            "\n",
            "know 0.015581835725102777 \n",
            "\n",
            "thing 0.015581835725102777 \n",
            "\n",
            "thought 0.014789538993317888 \n",
            "\n",
            "time 0.014789538993317888 \n",
            "\n",
            "see 0.014525440082722926 \n",
            "\n",
            "------------------ \n",
            "Chapter 10 top:\n",
            "\n",
            "said 0.08552990872263283 \n",
            "\n",
            "little 0.021979753079559274 \n",
            "\n",
            "like 0.018396097686152874 \n",
            "\n",
            "one 0.018157187326592446 \n",
            "\n",
            "went 0.016723725169229885 \n",
            "\n",
            "know 0.01576808373098818 \n",
            "\n",
            "thing 0.015051352652306896 \n",
            "\n",
            "could 0.014573531933186043 \n",
            "\n",
            "time 0.014573531933186043 \n",
            "\n",
            "thought 0.014334621573625616 \n",
            "\n",
            "------------------ \n",
            "Chapter 11 top:\n",
            "\n",
            "said 0.0874641315689189 \n",
            "\n",
            "little 0.02070939135384981 \n",
            "\n",
            "one 0.01894688996203281 \n",
            "\n",
            "like 0.017184388570215803 \n",
            "\n",
            "went 0.016082825200330175 \n",
            "\n",
            "know 0.015201574504421672 \n",
            "\n",
            "thought 0.014981261830444544 \n",
            "\n",
            "thing 0.014540636482490293 \n",
            "\n",
            "could 0.014320323808513167 \n",
            "\n",
            "time 0.014320323808513167 \n",
            "\n",
            "------------------ \n",
            "Chapter 12 top:\n",
            "\n",
            "said 0.12475453467173363 \n",
            "\n",
            "king 0.053466229145028694 \n",
            "\n",
            "jury 0.028006120028348364 \n",
            "\n",
            "little 0.02546010911668033 \n",
            "\n",
            "queen 0.0229140982050123 \n",
            "\n",
            "would 0.0229140982050123 \n",
            "\n",
            "one 0.020368087293344266 \n",
            "\n",
            "gave 0.01782207638167623 \n",
            "\n",
            "head 0.01782207638167623 \n",
            "\n",
            "important 0.01782207638167623 \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Proposed chapter's names:**\n",
        "\n",
        "\n",
        "1.   Little world\n",
        "2.   Mouse\n",
        "3.   Getting to know each other\n",
        "4.   Rabbit on the way\n",
        "5.   Time \n",
        "6.   Cat\n",
        "7.   Adventure of the friends Part I\n",
        "9.   ...Part II\n",
        "10.  ...Part III\n",
        "11.  ...Part IV\n",
        "11.  Adventure of the friends Part V\n",
        "12.  The battle\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Cma3XRbzOzut"
      },
      "id": "Cma3XRbzOzut"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**III. Find the Top 10 most used verbs in sentences with Alice. What does Alice do most often?**"
      ],
      "metadata": {
        "id": "pxxNTFJQSIrX"
      },
      "id": "pxxNTFJQSIrX"
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = re.split(r\"[.?!]\", raw_text)\n",
        "alice = []\n",
        "for s in sentences:\n",
        "  if 'Alice' in s:\n",
        "    alice.append(s)\n",
        "\n",
        "alice = str(alice).lower() #to lower-case\n",
        "alice = re.sub(\"\\d+\", \"\", str(alice))"
      ],
      "metadata": {
        "id": "RboBUQPwTUoQ"
      },
      "id": "RboBUQPwTUoQ",
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "txt = nlp(alice)\n",
        "\n",
        "verbs = []\n",
        "for token in txt:\n",
        "    if token.pos_ == \"VERB\":\n",
        "        verbs.append(token.text)\n",
        "top = Counter(verbs).most_common(10)\n",
        "for v, n in top:\n",
        "  print('Word:', v, '\\t\\t', 'Number of times:', n)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2I2uVQGOUwg7",
        "outputId": "df0c858f-b3c2-47eb-9564-6c2e820c9098"
      },
      "id": "2I2uVQGOUwg7",
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word: said \t\t Number of times: 166\n",
            "Word: thought \t\t Number of times: 53\n",
            "Word: alice \t\t Number of times: 34\n",
            "Word: went \t\t Number of times: 26\n",
            "Word: ’s \t\t Number of times: 26\n",
            "Word: had \t\t Number of times: 25\n",
            "Word: say \t\t Number of times: 21\n",
            "Word: see \t\t Number of times: 21\n",
            "Word: know \t\t Number of times: 21\n",
            "Word: do \t\t Number of times: 20\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.1"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}